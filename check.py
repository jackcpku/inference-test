from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import time
import torch
import math

# Define the model name
model_name = "Qwen/QwQ-32B"

# Load the tokenizer and model
print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
print("Tokenizer loaded successfully.")

print("Loading model (this may take some time due to model size)...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
print("Model loaded successfully.")

# Function to check if a response is likely generated by the LLM
def check_response_likelihood(model, tokenizer, prompt_messages, response_text, threshold):
    """
    Check if a response was likely generated by the model based on average diversion.
    
    Args:
        model: Loaded LLM model
        tokenizer: Corresponding tokenizer
        prompt_messages: List of message dicts for the prompt (e.g., [{"role": "user", "content": "..."}])
        response_text: String response to check
        threshold: Float threshold for average diversion (lower means more likely from model)
    
    Returns:
        bool: True if likely generated by the model, False otherwise
    """
    # Prepare full messages including response
    full_messages = prompt_messages + [{"role": "assistant", "content": response_text}]
    full_text = tokenizer.apply_chat_template(full_messages, tokenize=False)
    
    # Tokenize the full text
    full_inputs = tokenizer(full_text, return_tensors="pt").to(next(model.parameters()).device)
    input_ids = full_inputs.input_ids
    
    # Determine where the response starts by tokenizing the prompt alone
    prompt_text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)
    prompt_inputs = tokenizer(prompt_text, return_tensors="pt").to(next(model.parameters()).device)
    prompt_length = prompt_inputs.input_ids.shape[1]
    
    # Get logits for the entire sequence
    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)
    
    # Compute diversion for each token in the response part
    diversion_values = []
    for i in range(prompt_length - 1, input_ids.shape[1] - 1):
        logits_i = logits[0, i, :]  # Logits for predicting token i+1
        actual_token = input_ids[0, i + 1]  # The actual next token
        probs = torch.softmax(logits_i, dim=-1)  # Convert to probabilities
        p = probs[actual_token].item()  # Probability of the actual token
        diversion = -math.log(max(p, 1e-10))  # -log(p), clipped to avoid log(0)
        diversion_values.append(diversion)
    
    if not diversion_values:
        print("No response tokens to evaluate.")
        return False
    
    # Calculate average diversion
    average_diversion = sum(diversion_values) / len(diversion_values)
    print(f"Average diversion: {average_diversion:.4f}")
    
    # Compare to threshold
    return average_diversion < threshold

# Define the prompt and response to check
prompt = "How many r's are in the word \"strawberry\"?"
prompt_messages = [{"role": "user", "content": prompt}]

response_text = """<think>
Okay, so I need to figure out how many times the letter 'r' appears in the word "strawberry". Let me start by writing down the word and looking at each letter one by one. 

First, I'll spell out the word: S-T-R-A-W-B-E-R-R-Y. Wait, let me make sure I have that right. Sometimes people might miscount because of the way it's pronounced. Let me double-check the spelling. S-T-R-A-W-B-E-R-R-Y. Yeah, that's correct. Now I'll go through each letter slowly.

Starting with the first letter: S – that's an 'S', not an 'r'. Next is T – still no 'r' there. Then R – okay, that's the first 'r'. Got one so far. Next letter is A – no 'r'. Then W – nope. Next is B – still nothing. Then E – no 'r' there either. Now the next letter is R again – that's the second 'r'. Hmm, so that's two. Then another R – wait, so after E, it's R, R, and then Y? Let me confirm the spelling again to be sure. S-T-R-A-W-B-E-R-R-Y. Yes, after the E, there are two R's in a row, right? So that would make the third R? Wait, hold on. Let me count again step by step.

Breaking it down:

1. S
2. T
3. R (1st R)
4. A
5. W
6. B
7. E
8. R (2nd R)
9. R (3rd R)
10. Y

Wait, so after E comes two R's and then Y. So that's two R's after the E? So positions 8 and 9 are both R's, making the total count three R's? But hold on, maybe I'm miscounting the letters. Let me write them out with numbers:

1. S
2. T
3. R
4. A
5. W
6. B
7. E
8. R
9. R
10. Y

Yes, that's 10 letters in total. So positions 3, 8, and 9 are R's. So that's three R's? Wait, but when I first thought, I thought maybe two"""

response_text = """Haha
Okay, it is an interesting question! It's Monday today and I love working."""

# Test the response
threshold = 5.0  # Example threshold; calibrate this based on experiments
is_likely_generated = check_response_likelihood(model, tokenizer, prompt_messages, response_text, threshold)
print(f"Is the response likely generated by the model? {is_likely_generated}")